**系统架构 → 模块功能 → 技术细节 → 开发流程**。

---

# 💻 软件总体目标

* 在水晶球设备（Raspberry Pi 或小型主机）上运行，提供 **语音对话 + 虚拟人物表情/嘴型驱动**。
* 实现链路：**语音输入 → 语音识别（ASR） → LLM 对话 → 语音合成（TTS） → 动画渲染**。
* 用户能拿着球，对虚拟人物说话，人物会**动嘴、变表情并回复语音**。

---

# 1️⃣ 系统架构

```
用户语音 → ASR(语音转文字) → LLM(对话逻辑) → TTS(文字转语音) 
→ [输出1: 音频播放] + [输出2: viseme/能量驱动嘴型动画] → 圆形屏渲染人物
```

* **主控环境**：Raspberry Pi OS / Linux
* **核心语言**：Python（易于调用 API 和处理音频/图像）
* **并发模型**：异步 I/O（asyncio），保证音频处理与渲染不卡顿
* **进程管理**：systemd，开机自启、异常重启

---

# 2️⃣ 模块划分与核心技术

## 2.1 唤醒模块

* **基础版**：按键或触摸触发录音（最稳定）。
* **进阶版**：离线唤醒词识别（Porcupine、Snowboy），可喊“Hello Luna”激活。

## 2.2 语音采集与 ASR

* **录音工具**：`pyaudio` 或 `sounddevice` (Python)。
* **ASR 服务**（云端优先，保证准确率和低延迟）：

  * 微软 Azure Speech-to-Text
  * Google Cloud Speech-to-Text
  * 科大讯飞开放平台（国内可用）
* **数据流**：录音 PCM → 压缩 → 上传 → 返回文本
* **本地备选**：Whisper small/int8 模型（Pi 4 能跑，但延迟 >2s）。

## 2.3 LLM 对话模块

* **核心 API**：ChatGPT（gpt-4o-mini / gpt-4o） 或 国内大模型（通义千问、文心一言）。
* **角色设定**：在系统 prompt 里写死人物人设（如“温柔、治愈、初恋”）。
* **上下文管理**：SQLite 保存最近 N 轮对话，避免遗忘；超过则截断。
* **功能扩展**：情绪分析（LLM 输出 “joy/sadness/neutral”等标签），用于驱动表情切换。

## 2.4 TTS 模块

* **云端服务**（推荐）：

  * Azure TTS（支持 viseme 时间戳，精确嘴型同步）
  * ElevenLabs（音色自然，也支持 phoneme 序列）
* **本地备选**：espeak-ng（声音差，但延迟低）。
* **输出**：

  * 音频文件（wav/mp3）供播放；
  * Viseme/phoneme 时间轴 → 传给渲染模块驱动嘴型。

## 2.5 渲染模块

* **画布**：480×480 圆形区域（pygame/SDL 或 Unity 运行在 Linux）。
* **人物资源**：

  * 基础：一张背景图 + 多张嘴型/表情透明 PNG；
  * 进阶：Live2D Cubism SDK 或 Unity 动态角色。
* **嘴型驱动**：

  * 简易：用音频能量包络映射到嘴型（闭/半开/大开）。
  * 精细：根据 TTS 提供的 viseme 时间戳逐帧切换（更真实）。
* **表情控制**：根据 LLM 输出情绪标签切换对应表情（微笑、害羞、悲伤…）。

## 2.6 音频播放

* **播放库**：`pygame.mixer` / `pydub` / `ffplay` 调用。
* **AEC（回声消除）**：用 WebRTC Audio Processing 库，避免话筒录到喇叭声音。

## 2.7 数据与隐私

* **本地数据库**：SQLite 存储对话记录、人设设定。
* **隐私控制**：用户可手动清空对话数据。
* **内容安全**：调用云端审核 API 或本地敏感词过滤。

---

# 3️⃣ 开发流程（详细步骤）

### 第 1 步：环境准备

* 树莓派安装 Raspberry Pi OS
* 安装依赖：`python3-pyaudio sounddevice pygame sqlite3 requests`
* 配置音频输入输出（USB 麦 + 喇叭）

### 第 2 步：跑通语音链路

1. 录音 5 秒，保存为 wav；
2. 上传至 Azure Speech → 拿到文本；
3. 文本丢给 ChatGPT → 返回回复文本；
4. 回复文本交给 Azure TTS → 返回 wav 音频；
5. 播放音频，检查延迟（目标 1.5–3 s）。

### 第 3 步：嘴型同步

1. 先用简易能量驱动（检测音量大小切换嘴型 PNG）；
2. 升级为 TTS viseme → 帧动画（嘴型 A、O、E、M 对应时间点）。

### 第 4 步：表情控制

* 在 ChatGPT 输出 JSON：`{"reply": "...", "emotion": "joy"}`
* 根据 emotion 切换 PNG 表情层。

### 第 5 步：渲染整合

* 用 pygame 显示：背景 + 人物底图 + 嘴型帧叠加。
* 用 mask 裁剪成圆形画布，黑色背景，保证在球内显示效果清晰。

### 第 6 步：自动化

* 写 systemd service，开机自启：

  * 启动主程序，等待触发 → 完整语音对话闭环。

---

# 4️⃣ 技术难点与优化

* **延迟**：目标端到端 2 秒内 → 优先用云 ASR+TTS，Pi 只做渲染与播放。
* **嘴型同步**：能量驱动够 Demo，但要真实必须接 viseme/phoneme。
* **音质与啸叫**：AEC 必须开启，喇叭要远离麦克风。
* **功耗与散热**：长时间运行需加导热片，Pi 4 容易 70℃。

---

# 📌 总结

软件部分包含 **7 大模块**：唤醒、ASR、LLM、TTS、渲染、音频播放、数据隐私。
核心技术点是：

* **云端 ASR/TTS** 保证低延迟 + 高自然度；
* **viseme 驱动嘴型** 让人物动口说话；
* **LLM + Prompt** 控制角色人设和语气；
* **pygame/Live2D 渲染** 把动画显示在圆形屏上。

---
